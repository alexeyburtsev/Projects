{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install datasets","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-05-14T21:55:27.945967Z","iopub.execute_input":"2022-05-14T21:55:27.946580Z","iopub.status.idle":"2022-05-14T21:55:39.325066Z","shell.execute_reply.started":"2022-05-14T21:55:27.946481Z","shell.execute_reply":"2022-05-14T21:55:39.324137Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.27.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.63.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (7.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.3.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.5.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.12)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.8)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nimport random\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup,TrainingArguments, Trainer\nfrom datasets import load_metric\nfrom transformers import BertModel\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:55:39.327443Z","iopub.execute_input":"2022-05-14T21:55:39.327785Z","iopub.status.idle":"2022-05-14T21:55:47.393342Z","shell.execute_reply.started":"2022-05-14T21:55:39.327745Z","shell.execute_reply":"2022-05-14T21:55:47.392571Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:55:47.394530Z","iopub.execute_input":"2022-05-14T21:55:47.394782Z","iopub.status.idle":"2022-05-14T21:55:47.405016Z","shell.execute_reply.started":"2022-05-14T21:55:47.394748Z","shell.execute_reply":"2022-05-14T21:55:47.404123Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv\n/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv\n/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\ntest_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\nsub_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:06.608721Z","iopub.execute_input":"2022-05-14T21:56:06.608987Z","iopub.status.idle":"2022-05-14T21:56:06.706122Z","shell.execute_reply.started":"2022-05-14T21:56:06.608957Z","shell.execute_reply":"2022-05-14T21:56:06.705358Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Config:\n   # model\n    model = 'anferico/bert-for-patents'\n    \n    max_len = 32\n    num_epoch = 2\n    batch_size = 64\n    epochs = 2\n    lr = 1e-6","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:08.839552Z","iopub.execute_input":"2022-05-14T21:56:08.839990Z","iopub.status.idle":"2022-05-14T21:56:08.844327Z","shell.execute_reply.started":"2022-05-14T21:56:08.839954Z","shell.execute_reply":"2022-05-14T21:56:08.843604Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:09.067292Z","iopub.execute_input":"2022-05-14T21:56:09.068054Z","iopub.status.idle":"2022-05-14T21:56:09.076608Z","shell.execute_reply.started":"2022-05-14T21:56:09.068016Z","shell.execute_reply":"2022-05-14T21:56:09.075605Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():     \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\nprint( 'device set to =>', device)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:11.519455Z","iopub.execute_input":"2022-05-14T21:56:11.519826Z","iopub.status.idle":"2022-05-14T21:56:11.589525Z","shell.execute_reply.started":"2022-05-14T21:56:11.519792Z","shell.execute_reply":"2022-05-14T21:56:11.588481Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\ndevice set to => cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class PatentDataset( torch.utils.data.Dataset ):\n    def __init__( self, anchor, target, context, score, tokenizer, max_len ):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.score = score\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__( self ):\n        return len( self.anchor )\n    \n    def __getitem__( self, idx ):\n        anchor = self.anchor[idx ]\n        target = self.target[ idx ]\n        context = self.context[ idx ]\n        score = self.score[ idx ]\n        \n        encoded_data = self.tokenizer.encode_plus(\n                    context+ ' ' + anchor,\n                    target,\n                    padding = 'max_length',\n                    max_length = self.max_len,\n                    truncation=True,\n                    return_attention_mask = True,\n        )\n        input_ids = encoded_data[\"input_ids\"]\n        attention_mask = encoded_data[\"attention_mask\"]\n        token_type_ids = encoded_data[\"token_type_ids\"]\n        \n        return {\n            'input_ids' : torch.tensor( input_ids, dtype= torch.long),\n            'attention_mask': torch.tensor( attention_mask, dtype=torch.long),\n            'token_type': torch.tensor( token_type_ids, dtype=torch.long),\n            'label': torch.tensor( score, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:11.920935Z","iopub.execute_input":"2022-05-14T21:56:11.921469Z","iopub.status.idle":"2022-05-14T21:56:11.933793Z","shell.execute_reply.started":"2022-05-14T21:56:11.921432Z","shell.execute_reply":"2022-05-14T21:56:11.933009Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class PatentTestDataset( torch.utils.data.Dataset ):\n    def __init__( self, anchor, target, context, tokenizer, max_len ):\n        self.anchor = anchor\n        self.target = target\n        self.context = context\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__( self ):\n        return len( self.anchor )\n    \n    def __getitem__( self, idx ):\n        anchor = self.anchor[idx ]\n        target = self.target[ idx ]\n        context = self.context[ idx ]\n        \n        encoded_data = self.tokenizer.encode_plus(\n                    context+ ' ' + anchor,\n                    target,\n                    padding = 'max_length',\n                    max_length = self.max_len,\n                    truncation=True,\n                    return_attention_mask = True,\n        )\n        input_ids = encoded_data[\"input_ids\"]\n        attention_mask = encoded_data[\"attention_mask\"]\n        token_type_ids = encoded_data[\"token_type_ids\"]\n        \n        return {\n            'input_ids' : torch.tensor( input_ids, dtype= torch.long),\n            'attention_mask': torch.tensor( attention_mask, dtype=torch.long),\n            'token_type': torch.tensor( token_type_ids, dtype=torch.long),\n        }","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:16:25.222157Z","iopub.status.busy":"2022-05-06T17:16:25.221851Z","iopub.status.idle":"2022-05-06T17:16:25.233093Z","shell.execute_reply":"2022-05-06T17:16:25.2324Z","shell.execute_reply.started":"2022-05-06T17:16:25.222121Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__( self, dropout ):\n        super( BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(Config.model)\n        self.dropout = nn.Dropout( dropout )\n        self.linear = nn.Linear( 1024, 5 )\n        self.relu = nn.ReLU()\n        \n    def forward( self,  input_id, mask ):\n        _, pooled_output = self.bert( input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_output = self.dropout( pooled_output )\n        linear_output = self.linear( dropout_output )\n        final_layer = self.relu( linear_output )\n        return final_layer","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:21.051928Z","iopub.execute_input":"2022-05-14T21:56:21.052232Z","iopub.status.idle":"2022-05-14T21:56:21.061904Z","shell.execute_reply.started":"2022-05-14T21:56:21.052200Z","shell.execute_reply":"2022-05-14T21:56:21.061035Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained( Config.model , \n                                            padding='max_length',\n                                            pad_to_max_length = True,\n                                            max_length = Config.max_len,\n                                            truncation=True)   ","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:24.391708Z","iopub.execute_input":"2022-05-14T21:56:24.392223Z","iopub.status.idle":"2022-05-14T21:56:26.034155Z","shell.execute_reply.started":"2022-05-14T21:56:24.392183Z","shell.execute_reply":"2022-05-14T21:56:26.033348Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/327 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f31db79d5bd450a8b46556f590e0e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/322k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e603b82817943e38140f39310c96341"}},"metadata":{}}]},{"cell_type":"code","source":"train_examples = int(len(train_df) * 0.9)\ntrain_data = train_df.iloc[ :train_examples, 1:]\nval_data = train_df.iloc[ train_examples:, 1:]\nprint( 'train_size:', train_data.shape[0] )\nprint( 'val_size:', val_data.shape[0] )","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:26.035871Z","iopub.execute_input":"2022-05-14T21:56:26.036162Z","iopub.status.idle":"2022-05-14T21:56:26.051503Z","shell.execute_reply.started":"2022-05-14T21:56:26.036122Z","shell.execute_reply":"2022-05-14T21:56:26.049889Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"train_size: 32825\nval_size: 3648\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(model, train, val, learning_rate=.01, epochs=2):\n    train_dataset = PatentDataset(\n                    anchor = train.anchor.values,\n                    target = train.target.values,\n                    context = train.context.values,\n                    score = train.score.values,\n                    tokenizer = tokenizer,\n                    max_len = Config.max_len\n    )\n\n    val_dataset = PatentDataset(\n                        anchor = val.anchor.values,\n                        target = val.target.values,\n                        context = val.context.values,\n                        score = val.score.values,\n                        tokenizer = tokenizer,\n                        max_len = Config.max_len\n    )\n    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=32)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr= learning_rate)\n    \n    if torch.cuda.is_available():\n        model = model.cuda()\n        criterion = criterion.cuda()\n        \n    for epoch_num in range(epochs):\n        total_acc_train = 0\n        total_loss_train = 0\n        \n        for item in tqdm(train_dataloader):\n            train_label = item['label'].to(device)\n            mask = item['attention_mask'].to(device)\n            input_id = item['input_ids'].squeeze(1).to(device)\n\n\n            output = model(input_id, mask)\n            batch_loss = criterion(output, train_label)\n            total_loss_train += batch_loss.item()\n                \n            acc = (output.argmax(dim=1) == train_label).sum().item()\n            total_acc_train += acc\n            \n            \n            model.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n            \n            total_acc_val = 0\n            total_loss_val = 0\n\n        with torch.no_grad():\n\n            for item in val_dataloader:\n\n                val_label = item['label'].to(device)\n                mask = item['attention_mask'].to(device)\n                input_id = item['input_ids'].squeeze(1).to(device)\n\n                output = model(input_id, mask)\n\n                batch_loss = criterion(output, val_label)\n                total_loss_val += batch_loss.item()\n                    \n                acc = (output.argmax(dim=1) == val_label).sum().item()\n                total_acc_val += acc\n        save_path = f'bert_{epoch_num}.pt'\n        torch.save({\n            'epoch': epoch_num,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n#             'loss': LOSS,\n            }, save_path)    \n        print(\n                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n        print( 'model saved to =>', save_path)\n\nmodel = BertClassifier(dropout=0.5)\n           \ntrain(model, train_data, val_data, Config.lr, Config.epochs)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T21:56:28.332155Z","iopub.execute_input":"2022-05-14T21:56:28.332590Z","iopub.status.idle":"2022-05-14T22:11:06.783199Z","shell.execute_reply.started":"2022-05-14T21:56:28.332549Z","shell.execute_reply":"2022-05-14T22:11:06.782373Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1c4dcfb4194ce2b77e802ffe25cfbc"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n100%|██████████| 1026/1026 [06:25<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 1 | Train Loss:  0.006                 | Train Accuracy:  0.964                 | Val Loss:  0.002                 | Val Accuracy:  0.979\nmodel saved to => bert_0.pt\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1026/1026 [06:24<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 2 | Train Loss:  0.001                 | Train Accuracy:  0.989                 | Val Loss:  0.002                 | Val Accuracy:  0.986\nmodel saved to => bert_1.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, test_data):\n\n    test = PatentTestDataset( \n                    anchor = test_data.anchor.values,\n                    target = test_data.target.values,\n                    context = test_data.context.values,\n                    tokenizer = tokenizer,\n                    max_len = Config.max_len\n    )\n\n    test_dataloader = DataLoader(test, batch_size=32)\n\n    if torch.cuda.is_available():\n        model = model.cuda()\n    score_list = []\n    with torch.no_grad():\n\n        for item in test_dataloader:\n\n            mask = item['attention_mask'].to(device)\n            input_id = item['input_ids'].squeeze(1).to(device)\n            output = model(input_id, mask)\n            preds = output.argmax(dim=1)\n            score_list.append(  preds )\n\n    print( 'test generated =>' , len( score_list))\n    return score_list","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:31:08.860446Z","iopub.status.busy":"2022-05-06T17:31:08.860176Z","iopub.status.idle":"2022-05-06T17:31:08.873932Z","shell.execute_reply":"2022-05-06T17:31:08.873285Z","shell.execute_reply.started":"2022-05-06T17:31:08.860409Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = predict(model, test_df)","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:31:08.875504Z","iopub.status.busy":"2022-05-06T17:31:08.875224Z","iopub.status.idle":"2022-05-06T17:31:09.819691Z","shell.execute_reply":"2022-05-06T17:31:09.818896Z","shell.execute_reply.started":"2022-05-06T17:31:08.875449Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scorelist =[]\nfor item in scores:\n    scorelist.append(item.cpu().numpy())\npreds = np.hstack(scorelist)\nsub_df['preds'] =preds","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:31:09.821686Z","iopub.status.busy":"2022-05-06T17:31:09.821189Z","iopub.status.idle":"2022-05-06T17:31:10.841816Z","shell.execute_reply":"2022-05-06T17:31:10.840897Z","shell.execute_reply.started":"2022-05-06T17:31:09.821646Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:31:10.843438Z","iopub.status.busy":"2022-05-06T17:31:10.843184Z","iopub.status.idle":"2022-05-06T17:31:11.838115Z","shell.execute_reply":"2022-05-06T17:31:11.837307Z","shell.execute_reply.started":"2022-05-06T17:31:10.843403Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_map = dict(zip( range(5), ['0.00', '0.25', '0.50', '0.75', '1.00']))\ninverse_score_map = dict(zip( [0.00, 0.25, 0.50, 0.75, 1.00],range(5) ))","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:31:11.839845Z","iopub.status.busy":"2022-05-06T17:31:11.839568Z","iopub.status.idle":"2022-05-06T17:31:12.136832Z","shell.execute_reply":"2022-05-06T17:31:12.135866Z","shell.execute_reply.started":"2022-05-06T17:31:11.839805Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df['score']= sub_df.preds.astype(float).map(score_map)","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:31:12.139525Z","iopub.status.busy":"2022-05-06T17:31:12.138597Z","iopub.status.idle":"2022-05-06T17:31:12.15525Z","shell.execute_reply":"2022-05-06T17:31:12.154611Z","shell.execute_reply.started":"2022-05-06T17:31:12.139464Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os  \nos.makedirs('./kaggle/working', exist_ok=True)  ","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:31:12.156864Z","iopub.status.busy":"2022-05-06T17:31:12.156579Z","iopub.status.idle":"2022-05-06T17:31:12.16135Z","shell.execute_reply":"2022-05-06T17:31:12.160527Z","shell.execute_reply.started":"2022-05-06T17:31:12.156826Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[['id', 'score']].to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:38:24.396894Z","iopub.status.busy":"2022-05-06T17:38:24.396586Z","iopub.status.idle":"2022-05-06T17:38:24.405331Z","shell.execute_reply":"2022-05-06T17:38:24.403927Z","shell.execute_reply.started":"2022-05-06T17:38:24.396853Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2022-05-06T17:45:03.125979Z","iopub.status.busy":"2022-05-06T17:45:03.125281Z","iopub.status.idle":"2022-05-06T17:45:03.129756Z","shell.execute_reply":"2022-05-06T17:45:03.128756Z","shell.execute_reply.started":"2022-05-06T17:45:03.12594Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}